# DQN & DDQN Algorithms for Open-AI gym Cart pole
Implementation for DQN (Deep Q Network) and DDQN (Double Deep Q Networks) algorithms proposed in 

"Mnih, V., Kavukcuoglu, K., Silver, D. *et al.* Human-level control through deep reinforcement learning.                    *Nature* **518,** 529â€“533 (2015). https://doi.org/10.1038/nature14236"

and

"Hado van Hasselt, Arthur Guez, David Silver. Deep Reinforcement Learning with Double Q-learning https://arxiv.org/abs/1509.06461"

on Open-AI gym Cart Pole environment.

Also a fraction of pole's base distance to center and pole's angle from center were added as a cost in order to encourage model to keep the pole still and in center. Adding this short term cost should help agent to learn avoiding distance from center and increasing angle (which is the final goal) faster. Although removing these costs won't make it impossible for agent to learn, just makes it harder; This means training takes longer and agent's behaviour becomes less predictable and less stable.

Both methods of training create and save policy model in the same manner, therefore model parameters created by either one of training methods can be used for the Run file.



ğŸ“˜ README.md â€” CartPole DQN / DDQN Reinforcement Learning Project
ğŸ“Œ Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” OpenAI Gymì˜ CartPole-v1 í™˜ê²½ì—ì„œ
DQN(Deep Q-Network) ë° DDQN(Double DQN) ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ê³ ,
PyTorch ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµÂ·ì‹œì—°í•˜ëŠ” ê°•í™”í•™ìŠµ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

í•™ìŠµ(Training)ê³¼ ì‹œë®¬ë ˆì´ì…˜(Visualization)ì€
ì™„ì „íˆ ë¶„ë¦¬ëœ í™˜ê²½(Docker / Host venv) ì—ì„œ ì‹¤í–‰ë˜ë©°,
ì•ˆì •ì ì¸ ì¬í˜„ì„±ê³¼ GUI ë Œë”ë§ ì„±ëŠ¥ì„ ëª¨ë‘ í™•ë³´í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ“ Project Structure
CartPole-DQN-And-DDQN/
â”‚
â”œâ”€â”€ Train_DQN.py               # DQN í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ Train_DDQN.py              # DDQN í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ Model.py                   # ì‹ ê²½ë§ êµ¬ì¡° ì •ì˜
â”œâ”€â”€ play_dqn.py                # í•™ìŠµëœ DQN ì‹œë®¬ë ˆì´ì…˜
â”œâ”€â”€ play_ddqn.py               # í•™ìŠµëœ DDQN ì‹œë®¬ë ˆì´ì…˜
â”‚
â”œâ”€â”€ dqn_cartpole.pth           # í•™ìŠµ ì™„ë£Œëœ DQN ëª¨ë¸
â”œâ”€â”€ ddqn_cartpole.pth          # í•™ìŠµ ì™„ë£Œëœ DDQN ëª¨ë¸
â”‚
â””â”€â”€ README.md

ğŸ§© 1. í•™ìŠµ í™˜ê²½ / ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ë¶„ë¦¬ êµ¬ì¡°

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ í™˜ê²½ìœ¼ë¡œ ë‚˜ë‰˜ì–´ ì‹¤í–‰ë©ë‹ˆë‹¤.

ëª©ì 	í™˜ê²½	ë°©ì‹	ì„¤ëª…
Training (í•™ìŠµ)	Docker	docker run --gpus all ...	GPU ì•ˆì • ì‚¬ìš©, ì˜ì¡´ì„± ê³ ì •
Visualization (ì‹œë®¬ë ˆì´ì…˜)	Host Python venv	source vis_env/bin/activate	ì‹¤ì‹œê°„ CartPole GUI ë Œë”ë§
ğŸŸ¦ Simulation Environment (Host venv)

ì‹œë®¬ë ˆì´ì…˜ì€ GUI ë Œë”ë§ì´ í•„ìš”í•˜ë¯€ë¡œ
Ubuntu Host Python ê°€ìƒí™˜ê²½ì—ì„œ ì‹¤í–‰í•œë‹¤.

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source vis_env/bin/activate


ì´ í™˜ê²½ì—ëŠ” ë‹¤ìŒ íŒ¨í‚¤ì§€ê°€ í¬í•¨ëœë‹¤:

gymnasium

pygame

torch

numpy

ê¸°íƒ€ ì‹œë®¬ë ˆì´ì…˜ ê´€ë ¨ íŒ¨í‚¤ì§€

ì—¬ê¸°ì„œ í•™ìŠµ ì™„ë£Œëœ .pth ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì‹¤ì‹œê°„ CartPole ì œì–´ë¥¼ ì‹œì—°í•œë‹¤.

ğŸ³ Training Environment (Docker)

í•™ìŠµì€ Docker ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì‹¤í–‰ë˜ë©° GPUë¥¼ ì•ˆì •ì ìœ¼ë¡œ í™œìš©í•œë‹¤.

docker run -it --gpus all \
  -v $(pwd)/CartPole-DQN-And-DDQN:/app \
  cartpole-dqn-env


ì´ í™˜ê²½ì—ëŠ” ë‹¤ìŒ íŒ¨í‚¤ì§€ê°€ ê³ ì • ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ë¨:

gym==0.25.2 (DQN/DDQN ì½”ë“œì™€ í˜¸í™˜)

numpy<2.0

torch (CUDA ì§€ì›)

matplotlib, tqdm ë“±

ì»¨í…Œì´ë„ˆëŠ” í˜¸ìŠ¤íŠ¸ì˜ í”„ë¡œì íŠ¸ í´ë”(/app)ë¥¼ ê³µìœ í•˜ë¯€ë¡œ
í•™ìŠµ í›„ ìƒì„±ëœ .pth ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í˜¸ìŠ¤íŠ¸ì—ë„ ë™ê¸°í™”ëœë‹¤.

ğŸ”§ 2. Installation
âœ” 2.1 Clone Repository
git clone https://github.com/<user>/<repo>.git
cd CartPole-DQN-And-DDQN

âœ” 2.2 Create Simulation venv
python3 -m venv vis_env
source vis_env/bin/activate
pip install --upgrade pip
pip install torch gymnasium pygame numpy

âœ” 2.3 Build Docker Image (Training)

Dockerfile ì˜ˆì‹œ:

FROM python:3.10-slim

RUN apt-get update && apt-get install -y python3-opengl
RUN pip install --upgrade pip
RUN pip install "numpy<2.0" gym==0.25.2 torch matplotlib tqdm

WORKDIR /app
CMD ["/bin/bash"]


ì´ë¯¸ì§€ ë¹Œë“œ:

docker build -t cartpole-dqn-env .

ğŸ‹ï¸ 3. Training
â–¶ DQN í•™ìŠµ
docker run -it --gpus all \
  -v $(pwd)/CartPole-DQN-And-DDQN:/app \
  cartpole-dqn-env

python Train_DQN.py


ê²°ê³¼:

dqn_cartpole.pth ìƒì„±ë¨

â–¶ DDQN í•™ìŠµ
python Train_DDQN.py


ê²°ê³¼:

ddqn_cartpole.pth ìƒì„±ë¨
policy_net.pth (best test reward ê¸°ì¤€) ìƒì„±ë¨

ğŸ¬ 4. Simulation (Real-time Visualization)

Host ê°€ìƒí™˜ê²½ ì‹¤í–‰:

source vis_env/bin/activate

â–¶ DQN ì‹œì—°
python play_dqn.py


ì‹¤í–‰ í™”ë©´:

CartPole í™˜ê²½ì´ GUIë¡œ í‘œì‹œ

ì €ì¥ëœ DQN ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í–‰ë™ ì„ íƒ

episodeë³„ reward ì¶œë ¥

â–¶ DDQN ì‹œì—°
python play_ddqn.py

ğŸ“‚ 5. Model Files
íŒŒì¼ëª…	ì˜ë¯¸
dqn_cartpole.pth	DQN í•™ìŠµ ìµœì¢… ëª¨ë¸
ddqn_cartpole.pth	DDQN í•™ìŠµ ìµœì¢… ëª¨ë¸
policy_net.pth	DDQN í…ŒìŠ¤íŠ¸ ìµœê³ ì„±ëŠ¥ ëª¨ë¸(best test reward)