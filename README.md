# DQN & DDQN Algorithms for Open-AI gym Cart pole
Implementation for DQN (Deep Q Network) and DDQN (Double Deep Q Networks) algorithms proposed in 

"Mnih, V., Kavukcuoglu, K., Silver, D. *et al.* Human-level control through deep reinforcement learning.                    *Nature* **518,** 529â€“533 (2015). https://doi.org/10.1038/nature14236"

and

"Hado van Hasselt, Arthur Guez, David Silver. Deep Reinforcement Learning with Double Q-learning https://arxiv.org/abs/1509.06461"

on Open-AI gym Cart Pole environment.

Also a fraction of pole's base distance to center and pole's angle from center were added as a cost in order to encourage model to keep the pole still and in center. Adding this short term cost should help agent to learn avoiding distance from center and increasing angle (which is the final goal) faster. Although removing these costs won't make it impossible for agent to learn, just makes it harder; This means training takes longer and agent's behaviour becomes less predictable and less stable.

Both methods of training create and save policy model in the same manner, therefore model parameters created by either one of training methods can be used for the Run file.



ğŸ“˜ CartPole DQN / DDQN Reinforcement Learning Project
ğŸ“Œ Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” OpenAI Gym CartPole-v1 í™˜ê²½ì—ì„œ
DQN(Deep Q-Network) ë° DDQN(Double DQN) ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ê³ 
PyTorch ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ(Training) ë° ì‹œê°í™”(Simulation) ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê°•í™”í•™ìŠµ í”„ë¡œì íŠ¸ì´ë‹¤.

íŠ¹íˆ í•™ìŠµ í™˜ê²½(Docker) ê³¼ ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½(Host venv) ì„ ì™„ì „íˆ ë¶„ë¦¬í•˜ì—¬

ì•ˆì •ì ì¸ ì˜ì¡´ì„±

GPU ê¸°ë°˜ í•™ìŠµ

ì‹¤ì‹œê°„ GUI ë Œë”ë§

ì„ ëª¨ë‘ ë§Œì¡±í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆë‹¤.

ğŸ“ Project Structure
CartPole-DQN-And-DDQN/
â”‚
â”œâ”€â”€ Train_DQN.py          # DQN í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ Train_DDQN.py         # DDQN í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ Model.py              # ì‹ ê²½ë§ êµ¬ì¡° ì •ì˜
â”œâ”€â”€ play_dqn.py           # í•™ìŠµëœ DQN ì‹œì—°
â”œâ”€â”€ play_ddqn.py          # í•™ìŠµëœ DDQN ì‹œì—°
â”‚
â”œâ”€â”€ dqn_cartpole.pth      # í•™ìŠµ ì™„ë£Œ DQN ëª¨ë¸
â”œâ”€â”€ ddqn_cartpole.pth     # í•™ìŠµ ì™„ë£Œ DDQN ëª¨ë¸
â”œâ”€â”€ policy_net.pth        # DDQN ìµœê³ ì„±ëŠ¥(best) ì •ì±… ë„¤íŠ¸ì›Œí¬
â”‚
â””â”€â”€ README.md

ğŸ§© 1. í•™ìŠµ í™˜ê²½ / ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ë¶„ë¦¬ êµ¬ì¡°

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ë‘ í™˜ê²½ì—ì„œ ë™ì‘í•œë‹¤.

ëª©ì 	í™˜ê²½	ë°©ì‹	ì„¤ëª…
Training	Docker	docker run ...	GPU í•™ìŠµ / ì˜ì¡´ì„± ê³ ì •
Visualization	Host Python venv	source vis_env/bin/activate	ì‹¤ì‹œê°„ ë Œë”ë§, GUI í‘œì‹œ
ğŸŸ¦ Simulation Environment (Host venv)

CartPole GUI ë Œë”ë§ì€ Docker X11 ì œì•½ì„ í”¼í•˜ê¸° ìœ„í•´
Ubuntu Host Python virtualenvì—ì„œ ì‹¤í–‰í•œë‹¤.

ê°€ìƒí™˜ê²½ í™œì„±í™”
source vis_env/bin/activate

ì„¤ì¹˜ë˜ëŠ” ì£¼ìš” íŒ¨í‚¤ì§€

gymnasium

torch

numpy

pygame

ê¸°íƒ€ ë Œë”ë§ ê´€ë ¨ íŒ¨í‚¤ì§€

ì´ í™˜ê²½ì—ì„œ í•™ìŠµëœ .pth ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì‹¤ì‹œê°„ ê²Œì„ í”Œë ˆì´ ì‹œì—°ì„ í•œë‹¤.

ğŸ³ Training Environment (Docker)

í•™ìŠµì€ Docker ì»¨í…Œì´ë„ˆì—ì„œ ìˆ˜í–‰í•˜ë©°, GPUë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

í•™ìŠµìš© ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -it --gpus all \
  -v $(pwd)/CartPole-DQN-And-DDQN:/app \
  cartpole-dqn-env


Docker ë‚´ë¶€ì—ëŠ” ë‹¤ìŒ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤:

gym==0.25.2 (DQN/DDQN ì½”ë“œì™€ í˜¸í™˜)

numpy<2.0

PyTorch (CUDA)

matplotlib, tqdm ë“±

ëª¨ë¸ì´ ì €ì¥ë˜ëŠ” /app/*.pth íŒŒì¼ì€ í˜¸ìŠ¤íŠ¸ì—ë„ ìë™ ë°˜ì˜ëœë‹¤.

ğŸ”§ 2. Installation
âœ” 2.1 Clone Repository
git clone https://github.com/<your-id>/CartPole-DQN-And-DDQN.git
cd CartPole-DQN-And-DDQN

âœ” 2.2 Create Simulation venv
python3 -m venv vis_env
source vis_env/bin/activate

pip install --upgrade pip
pip install torch gymnasium pygame numpy

âœ” 2.3 Build Docker Image (Training)
Dockerfile ì˜ˆì‹œ
FROM python:3.10-slim

RUN apt-get update && apt-get install -y python3-opengl

RUN pip install --upgrade pip
RUN pip install "numpy<2.0" gym==0.25.2 torch matplotlib tqdm

WORKDIR /app
CMD ["/bin/bash"]

ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t cartpole-dqn-env .

ğŸ‹ï¸ 3. Training
â–¶ DQN í•™ìŠµ
docker run -it --gpus all \
  -v $(pwd)/CartPole-DQN-And-DDQN:/app \
  cartpole-dqn-env


ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ:

python Train_DQN.py


â¡ ìƒì„± íŒŒì¼: dqn_cartpole.pth

â–¶ DDQN í•™ìŠµ

ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ:

python Train_DDQN.py


â¡ ìƒì„± íŒŒì¼:

ddqn_cartpole.pth

policy_net.pth (best reward)

ğŸ¬ 4. Simulation (Real-time Visualization)
í˜¸ìŠ¤íŠ¸ ê°€ìƒí™˜ê²½ ì‹¤í–‰
source vis_env/bin/activate

â–¶ DQN ì‹œì—°
python play_dqn.py

â–¶ DDQN ì‹œì—°
python play_ddqn.py

ğŸ“‚ 5. Model Files
íŒŒì¼ëª…	ì„¤ëª…
dqn_cartpole.pth	DQN í•™ìŠµ ìµœì¢… ëª¨ë¸
ddqn_cartpole.pth	DDQN í•™ìŠµ ìµœì¢… ëª¨ë¸
policy_net.pth	DDQN ìµœê³ ì„±ëŠ¥ ëª¨ë¸(best policy)