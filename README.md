# DQN & DDQN Algorithms for Open-AI gym Cart pole
Implementation for DQN (Deep Q Network) and DDQN (Double Deep Q Networks) algorithms proposed in 

"Mnih, V., Kavukcuoglu, K., Silver, D. *et al.* Human-level control through deep reinforcement learning.                    *Nature* **518,** 529â€“533 (2015). https://doi.org/10.1038/nature14236"

and

"Hado van Hasselt, Arthur Guez, David Silver. Deep Reinforcement Learning with Double Q-learning https://arxiv.org/abs/1509.06461"

on Open-AI gym Cart Pole environment.

Also a fraction of pole's base distance to center and pole's angle from center were added as a cost in order to encourage model to keep the pole still and in center. Adding this short term cost should help agent to learn avoiding distance from center and increasing angle (which is the final goal) faster. Although removing these costs won't make it impossible for agent to learn, just makes it harder; This means training takes longer and agent's behaviour becomes less predictable and less stable.

Both methods of training create and save policy model in the same manner, therefore model parameters created by either one of training methods can be used for the Run file.



-----

# ğŸ“˜ CartPole DQN / DDQN Reinforcement Learning Project

## ğŸ“Œ Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” **OpenAI Gym CartPole-v1** í™˜ê²½ì—ì„œ **DQN(Deep Q-Network)** ë° **DDQN(Double DQN)** ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ê³ , PyTorchë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ(Training) ë° ì‹œê°í™”(Simulation)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê°•í™”í•™ìŠµ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

íŠ¹íˆ **í•™ìŠµ í™˜ê²½(Docker)**ê³¼ **ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½(Host venv)**ì„ ì™„ì „íˆ ë¶„ë¦¬í•˜ì—¬ ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

1.  **ì•ˆì •ì ì¸ ì˜ì¡´ì„± ê´€ë¦¬** (Environment Isolation)
2.  **GPU ê¸°ë°˜ì˜ ê³ ì† í•™ìŠµ** (CUDA Support)
3.  **ì‹¤ì‹œê°„ GUI ë Œë”ë§** (Real-time Visualization)

-----

## ğŸ“ Project Structure

```text
CartPole-DQN-And-DDQN/
â”‚
â”œâ”€â”€ Train_DQN.py          # ğŸ§  DQN í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ Train_DDQN.py         # ğŸ§  DDQN í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ Model.py              # ğŸ—ï¸ ì‹ ê²½ë§ êµ¬ì¡° ì •ì˜ (Network Architecture)
â”œâ”€â”€ play_dqn.py           # ğŸ¬ í•™ìŠµëœ DQN ì‹œì—° (Inference)
â”œâ”€â”€ play_ddqn.py          # ğŸ¬ í•™ìŠµëœ DDQN ì‹œì—° (Inference)
â”‚
â”œâ”€â”€ dqn_cartpole.pth      # ğŸ’¾ í•™ìŠµ ì™„ë£Œ DQN ëª¨ë¸
â”œâ”€â”€ ddqn_cartpole.pth     # ğŸ’¾ í•™ìŠµ ì™„ë£Œ DDQN ëª¨ë¸
â”œâ”€â”€ policy_net.pth        # ğŸ† DDQN ìµœê³  ì„±ëŠ¥(Best Reward) ì •ì±… ë„¤íŠ¸ì›Œí¬
â”‚
â””â”€â”€ README.md
```

-----

## ğŸ§© Architecture: í•™ìŠµ/ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ë¶„ë¦¬

ë³¸ í”„ë¡œì íŠ¸ëŠ” ëª©ì ì— ë”°ë¼ ë‘ ê°€ì§€ í™˜ê²½ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.

| ëª©ì  | í™˜ê²½ (Environment) | ì‹¤í–‰ ë°©ì‹ | ì£¼ìš” íŠ¹ì§• |
| :--- | :--- | :--- | :--- |
| **Training** | ğŸ³ **Docker Container** | `docker run ...` | GPU í•™ìŠµ, ì˜ì¡´ì„± ê³ ì • (`gym==0.25.2`) |
| **Visualization** | ğŸ–¥ï¸ **Host Python venv** | `source vis_env/bin/activate` | ì‹¤ì‹œê°„ ë Œë”ë§, GUI í‘œì‹œ (`gymnasium`) |

### ğŸŸ¦ 1. Simulation Environment (Host venv)

CartPole GUI ë Œë”ë§ì€ Docker ë‚´ë¶€ì˜ X11 ì œì•½ì„ í”¼í•˜ê¸° ìœ„í•´ **Ubuntu Hostì˜ Python virtualenv**ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

  * **ê°€ìƒí™˜ê²½ í™œì„±í™”:**
    ```bash
    source vis_env/bin/activate
    ```
  * **ì£¼ìš” íŒ¨í‚¤ì§€:**
      * `gymnasium` (ìµœì‹  ë Œë”ë§ ì§€ì›)
      * `torch`, `numpy`
      * `pygame` (ë Œë”ë§ ë°±ì—”ë“œ)
  * ì´ í™˜ê²½ì—ì„œëŠ” í•™ìŠµëœ `.pth` ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ **ì‹¤ì‹œê°„ ê²Œì„ í”Œë ˆì´**ë¥¼ ì‹œì—°í•©ë‹ˆë‹¤.

### ğŸ³ 2. Training Environment (Docker)

í•™ìŠµì€ **Docker ì»¨í…Œì´ë„ˆ** ë‚´ë¶€ì—ì„œ ìˆ˜í–‰í•˜ë©°, GPUë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

  * **ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ëª…ë ¹ì–´:**
    ```bash
    docker run -it --gpus all \
      -v $(pwd)/CartPole-DQN-And-DDQN:/app \
      cartpole-dqn-env
    ```
  * **ì£¼ìš” íŒ¨í‚¤ì§€:**
      * `gym==0.25.2` (DQN/DDQN ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€)
      * `numpy<2.0`
      * `PyTorch` (CUDA ì§€ì›)
      * `matplotlib`, `tqdm`
  * ëª¨ë¸ì´ ì €ì¥ë˜ëŠ” `/app/*.pth` íŒŒì¼ì€ ë³¼ë¥¨ ë§ˆìš´íŠ¸(`-v`)ë¥¼ í†µí•´ **í˜¸ìŠ¤íŠ¸ ê²½ë¡œì—ë„ ìë™ ë°˜ì˜**ë©ë‹ˆë‹¤.

-----

## ğŸ”§ Installation

### 1\. Clone Repository

```bash
git clone https://github.com/<your-id>/CartPole-DQN-And-DDQN.git
cd CartPole-DQN-And-DDQN
```

### 2\. Create Simulation venv (Host)

ì‹œê°í™”ë¥¼ ìœ„í•œ í˜¸ìŠ¤íŠ¸ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.

```bash
python3 -m venv vis_env
source vis_env/bin/activate

pip install --upgrade pip
pip install torch gymnasium pygame numpy
```

### 3\. Build Docker Image (Training)

í•™ìŠµìš© ë„ì»¤ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.

**Dockerfile Example:**

```dockerfile
FROM python:3.10-slim

RUN apt-get update && apt-get install -y python3-opengl

RUN pip install --upgrade pip
RUN pip install "numpy<2.0" gym==0.25.2 torch matplotlib tqdm

WORKDIR /app
CMD ["/bin/bash"]
```

**Build Command:**

```bash
docker build -t cartpole-dqn-env .
```

-----

## ğŸ‹ï¸ Training (in Docker)

í•™ìŠµì€ ë„ì»¤ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì§„í–‰í•©ë‹ˆë‹¤.

1.  **ì»¨í…Œì´ë„ˆ ì§„ì…:**

    ```bash
    docker run -it --gpus all \
      -v $(pwd)/CartPole-DQN-And-DDQN:/app \
      cartpole-dqn-env
    ```

2.  **DQN í•™ìŠµ:**

    ```bash
    python Train_DQN.py
    # â¡ ìƒì„± íŒŒì¼: dqn_cartpole.pth
    ```

3.  **DDQN í•™ìŠµ:**

    ```bash
    python Train_DDQN.py
    # â¡ ìƒì„± íŒŒì¼: ddqn_cartpole.pth, policy_net.pth (best)
    ```

-----

## ğŸ¬ Simulation (in Host)

í•™ìŠµëœ ëª¨ë¸ì„ ëˆˆìœ¼ë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ í˜¸ìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

1.  **ê°€ìƒí™˜ê²½ í™œì„±í™”:**

    ```bash
    source vis_env/bin/activate
    ```

2.  **DQN ì‹œì—°, DDQN ì‹œì—°:**

    ```bash
    python Run.py
    ```

-----

## ğŸ“‚ Model Files

| íŒŒì¼ëª… | ì„¤ëª… | ë¹„ê³  |
| :--- | :--- | :--- |
| `dqn_cartpole.pth` | DQN í•™ìŠµ ìµœì¢… ëª¨ë¸ | |
| `ddqn_cartpole.pth` | DDQN í•™ìŠµ ìµœì¢… ëª¨ë¸ | |
| `policy_net.pth` | DDQN ìµœê³  ì„±ëŠ¥ ëª¨ë¸ | **Best Reward** ë‹¬ì„± ì‹œ ì €ì¥ë¨ |